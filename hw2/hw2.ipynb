{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e07311d-f434-4a5d-8c81-b2804f00421a",
   "metadata": {},
   "source": [
    "![q](pic/q1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112691-a89a-4c1d-bbc4-3dbefcb2e063",
   "metadata": {},
   "source": [
    "![s](pic/q1a_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae88f85-3c2e-470a-96f5-01aad36068dc",
   "metadata": {},
   "source": [
    "![s](pic/q1a_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5033cc-b961-43c7-8a32-e043834d00ea",
   "metadata": {},
   "source": [
    "![s](pic/q1a_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6c5a0-a467-44f7-877a-d242746c3651",
   "metadata": {},
   "source": [
    "![q](pic/q1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e5628-ee0b-476e-af0c-cb963ef3c3fb",
   "metadata": {},
   "source": [
    "![s](pic/q1a_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c00a9-437a-4ef9-86ad-f8a538acf24a",
   "metadata": {},
   "source": [
    "![s](pic/q1a_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fb389-e7ec-4155-bbad-c879cc03c985",
   "metadata": {},
   "source": [
    "![q](pic/q2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7678da82-8805-476d-b4ba-a72068fe8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] Train Loss: 1.5172 Val Loss: 1.1962\n",
      "Epoch [20/1000] Train Loss: 0.6869 Val Loss: 0.9760\n",
      "Epoch [30/1000] Train Loss: 1.1388 Val Loss: 0.8555\n",
      "Epoch [40/1000] Train Loss: 0.5846 Val Loss: 0.7718\n",
      "Epoch [50/1000] Train Loss: 0.6386 Val Loss: 0.7104\n",
      "Epoch [60/1000] Train Loss: 0.7176 Val Loss: 0.6610\n",
      "Epoch [70/1000] Train Loss: 0.3893 Val Loss: 0.6227\n",
      "Epoch [80/1000] Train Loss: 0.5077 Val Loss: 0.5948\n",
      "Epoch [90/1000] Train Loss: 0.4340 Val Loss: 0.5717\n",
      "Epoch [100/1000] Train Loss: 0.4538 Val Loss: 0.5547\n",
      "Epoch [110/1000] Train Loss: 0.3437 Val Loss: 0.5422\n",
      "Epoch [120/1000] Train Loss: 0.4854 Val Loss: 0.5331\n",
      "Epoch [130/1000] Train Loss: 0.4482 Val Loss: 0.5260\n",
      "Epoch [140/1000] Train Loss: 0.4762 Val Loss: 0.5206\n",
      "Epoch [150/1000] Train Loss: 0.4783 Val Loss: 0.5161\n",
      "Epoch [160/1000] Train Loss: 0.3977 Val Loss: 0.5129\n",
      "Epoch [170/1000] Train Loss: 0.4017 Val Loss: 0.5109\n",
      "Epoch [180/1000] Train Loss: 0.5850 Val Loss: 0.5080\n",
      "Epoch [190/1000] Train Loss: 0.6187 Val Loss: 0.5056\n",
      "Epoch [200/1000] Train Loss: 0.5244 Val Loss: 0.5044\n",
      "Epoch [210/1000] Train Loss: 0.4081 Val Loss: 0.5030\n",
      "Epoch [220/1000] Train Loss: 0.4726 Val Loss: 0.5017\n",
      "Epoch [230/1000] Train Loss: 0.4519 Val Loss: 0.5016\n",
      "Epoch [240/1000] Train Loss: 0.3806 Val Loss: 0.5007\n",
      "Epoch [250/1000] Train Loss: 0.6588 Val Loss: 0.5007\n",
      "Epoch [260/1000] Train Loss: 0.4570 Val Loss: 0.4989\n",
      "Epoch [270/1000] Train Loss: 0.6000 Val Loss: 0.5002\n",
      "Epoch [280/1000] Train Loss: 0.2656 Val Loss: 0.4980\n",
      "Epoch [290/1000] Train Loss: 0.3512 Val Loss: 0.4972\n",
      "Epoch [300/1000] Train Loss: 0.7240 Val Loss: 0.4979\n",
      "Epoch [310/1000] Train Loss: 0.5565 Val Loss: 0.4985\n",
      "Epoch [320/1000] Train Loss: 0.2863 Val Loss: 0.4954\n",
      "Epoch [330/1000] Train Loss: 0.3110 Val Loss: 0.4960\n",
      "Epoch [340/1000] Train Loss: 0.5819 Val Loss: 0.4974\n",
      "Epoch [350/1000] Train Loss: 0.4513 Val Loss: 0.4967\n",
      "Epoch [360/1000] Train Loss: 0.4190 Val Loss: 0.4952\n",
      "Epoch [370/1000] Train Loss: 0.3326 Val Loss: 0.4970\n",
      "Epoch [380/1000] Train Loss: 0.6557 Val Loss: 0.4942\n",
      "Epoch [390/1000] Train Loss: 0.3600 Val Loss: 0.4953\n",
      "Epoch [400/1000] Train Loss: 0.6058 Val Loss: 0.4955\n",
      "Epoch [410/1000] Train Loss: 0.4028 Val Loss: 0.4962\n",
      "Epoch [420/1000] Train Loss: 0.6134 Val Loss: 0.4946\n",
      "Epoch [430/1000] Train Loss: 0.4110 Val Loss: 0.4939\n",
      "Epoch [440/1000] Train Loss: 0.4649 Val Loss: 0.4946\n",
      "Epoch [450/1000] Train Loss: 0.2562 Val Loss: 0.4950\n",
      "Epoch [460/1000] Train Loss: 0.3555 Val Loss: 0.4929\n",
      "Epoch [470/1000] Train Loss: 0.4106 Val Loss: 0.4925\n",
      "Epoch [480/1000] Train Loss: 0.4897 Val Loss: 0.4944\n",
      "Epoch [490/1000] Train Loss: 0.6872 Val Loss: 0.4934\n",
      "Epoch [500/1000] Train Loss: 0.5287 Val Loss: 0.4933\n",
      "Epoch [510/1000] Train Loss: 0.5904 Val Loss: 0.4924\n",
      "Epoch [520/1000] Train Loss: 0.4433 Val Loss: 0.4919\n",
      "Epoch [530/1000] Train Loss: 0.3985 Val Loss: 0.4924\n",
      "Epoch [540/1000] Train Loss: 0.4025 Val Loss: 0.4917\n",
      "Epoch [550/1000] Train Loss: 0.4826 Val Loss: 0.4918\n",
      "Epoch [560/1000] Train Loss: 0.5556 Val Loss: 0.4925\n",
      "Epoch [570/1000] Train Loss: 0.5454 Val Loss: 0.4919\n",
      "Epoch [580/1000] Train Loss: 0.3088 Val Loss: 0.4916\n",
      "Epoch [590/1000] Train Loss: 0.4755 Val Loss: 0.4917\n",
      "Epoch [600/1000] Train Loss: 0.6195 Val Loss: 0.4922\n",
      "Epoch [610/1000] Train Loss: 0.4396 Val Loss: 0.4910\n",
      "Epoch [620/1000] Train Loss: 0.4336 Val Loss: 0.4913\n",
      "Epoch [630/1000] Train Loss: 0.3933 Val Loss: 0.4916\n",
      "Epoch [640/1000] Train Loss: 0.5666 Val Loss: 0.4914\n",
      "Epoch [650/1000] Train Loss: 0.3979 Val Loss: 0.4932\n",
      "Epoch [660/1000] Train Loss: 0.3765 Val Loss: 0.4925\n",
      "Epoch [670/1000] Train Loss: 0.4798 Val Loss: 0.4916\n",
      "Epoch [680/1000] Train Loss: 0.4466 Val Loss: 0.4903\n",
      "Epoch [690/1000] Train Loss: 0.4098 Val Loss: 0.4916\n",
      "Epoch [700/1000] Train Loss: 0.4049 Val Loss: 0.4914\n",
      "Epoch [710/1000] Train Loss: 0.3288 Val Loss: 0.4908\n",
      "Epoch [720/1000] Train Loss: 0.4118 Val Loss: 0.4902\n",
      "Epoch [730/1000] Train Loss: 0.2536 Val Loss: 0.4908\n",
      "Epoch [740/1000] Train Loss: 0.3962 Val Loss: 0.4903\n",
      "Epoch [750/1000] Train Loss: 0.4930 Val Loss: 0.4919\n",
      "Epoch [760/1000] Train Loss: 0.4647 Val Loss: 0.4911\n",
      "Epoch [770/1000] Train Loss: 0.3252 Val Loss: 0.4926\n",
      "Epoch [780/1000] Train Loss: 0.4492 Val Loss: 0.4910\n",
      "Epoch [790/1000] Train Loss: 0.3648 Val Loss: 0.4905\n",
      "Epoch [800/1000] Train Loss: 0.5251 Val Loss: 0.4909\n",
      "Epoch [810/1000] Train Loss: 0.5680 Val Loss: 0.4897\n",
      "Epoch [820/1000] Train Loss: 0.5083 Val Loss: 0.4908\n",
      "Epoch [830/1000] Train Loss: 0.5102 Val Loss: 0.4909\n",
      "Epoch [840/1000] Train Loss: 0.3416 Val Loss: 0.4908\n",
      "Epoch [850/1000] Train Loss: 0.3401 Val Loss: 0.4915\n",
      "Epoch [860/1000] Train Loss: 0.3955 Val Loss: 0.4903\n",
      "Epoch [870/1000] Train Loss: 0.4150 Val Loss: 0.4897\n",
      "Epoch [880/1000] Train Loss: 0.5082 Val Loss: 0.4893\n",
      "Epoch [890/1000] Train Loss: 0.4316 Val Loss: 0.4892\n",
      "Epoch [900/1000] Train Loss: 0.5905 Val Loss: 0.4900\n",
      "Epoch [910/1000] Train Loss: 0.4409 Val Loss: 0.4895\n",
      "Epoch [920/1000] Train Loss: 0.3502 Val Loss: 0.4915\n",
      "Epoch [930/1000] Train Loss: 0.4853 Val Loss: 0.4894\n",
      "Epoch [940/1000] Train Loss: 0.3245 Val Loss: 0.4893\n",
      "Epoch [950/1000] Train Loss: 0.4639 Val Loss: 0.4903\n",
      "Epoch [960/1000] Train Loss: 0.3129 Val Loss: 0.4907\n",
      "Epoch [970/1000] Train Loss: 0.5138 Val Loss: 0.4892\n",
      "Epoch [980/1000] Train Loss: 0.4983 Val Loss: 0.4899\n",
      "Epoch [990/1000] Train Loss: 0.6390 Val Loss: 0.4911\n",
      "Epoch [1000/1000] Train Loss: 0.4742 Val Loss: 0.4887\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4980\n",
      "Mean Absolute Error (MAE): 0.5522\n",
      "Root Mean Squared Error (RMSE): 0.7057\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# Load the UCI Wine dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "wine_data = pd.read_csv(url, sep=\";\")\n",
    "wine_features = wine_data.drop(columns=[\"quality\"])\n",
    "wine_target = wine_data[\"quality\"]\n",
    "\n",
    "# Standardize features, data pre-processing\n",
    "scaler = StandardScaler() ## ??\n",
    "wine_features = scaler.fit_transform(wine_features) ## ??\n",
    "\n",
    "# Split the data into training, validation, and testing sets (64-16-20 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(wine_features, wine_target, test_size=0.36, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5556, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.FloatTensor(y_val.values).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class WineQualityRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(WineQualityRegressor, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "model = WineQualityRegressor(input_size, hidden_size)\n",
    "\n",
    "# Define custom batch size and learning rate\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "    train_loss_history.append(loss.item())\n",
    "    val_loss_history.append(val_loss.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {loss.item():.4f} Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.FloatTensor(X_test))\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    mae = mean_absolute_error(y_test, test_outputs.numpy())\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916019c-b09c-4e8b-97bd-3f8edb33308c",
   "metadata": {},
   "source": [
    "I think my model fits the data well, because the final value of loss is very small.\n",
    "I think it is not good to use continuous label to treat the wine quality because I think the wine quality label should be treated as classification label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9a058-88da-4a71-8660-3f9fdea67f52",
   "metadata": {},
   "source": [
    "![q](pic/q3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dd22f79-8409-40a0-95f5-0464245b788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD_gamma0.1_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4499 Val Loss: 0.4958\n",
      "Epoch [100/300] Train Loss: 0.5511 Val Loss: 0.4943\n",
      "Epoch [150/300] Train Loss: 0.3772 Val Loss: 0.4943\n",
      "Epoch [200/300] Train Loss: 0.5118 Val Loss: 0.4943\n",
      "Epoch [250/300] Train Loss: 0.5664 Val Loss: 0.4943\n",
      "Epoch [300/300] Train Loss: 0.4212 Val Loss: 0.4943\n",
      "Training SGD_gamma0.1_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.6702 Val Loss: 0.5022\n",
      "Epoch [100/300] Train Loss: 0.5955 Val Loss: 0.5015\n",
      "Epoch [150/300] Train Loss: 0.4030 Val Loss: 0.5014\n",
      "Epoch [200/300] Train Loss: 0.4666 Val Loss: 0.5014\n",
      "Epoch [250/300] Train Loss: 0.4319 Val Loss: 0.5014\n",
      "Epoch [300/300] Train Loss: 0.4294 Val Loss: 0.5014\n",
      "Training SGD_gamma0.1_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.4555 Val Loss: 0.4870\n",
      "Epoch [100/300] Train Loss: 0.4017 Val Loss: 0.4864\n",
      "Epoch [150/300] Train Loss: 0.5282 Val Loss: 0.4864\n",
      "Epoch [200/300] Train Loss: 0.4618 Val Loss: 0.4864\n",
      "Epoch [250/300] Train Loss: 0.6337 Val Loss: 0.4864\n",
      "Epoch [300/300] Train Loss: 0.4226 Val Loss: 0.4864\n",
      "Training SGD_gamma0.1_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.5769 Val Loss: 0.7944\n",
      "Epoch [100/300] Train Loss: 1.0632 Val Loss: 0.7817\n",
      "Epoch [150/300] Train Loss: 0.4402 Val Loss: 0.7815\n",
      "Epoch [200/300] Train Loss: 0.4685 Val Loss: 0.7815\n",
      "Epoch [250/300] Train Loss: 1.2270 Val Loss: 0.7815\n",
      "Epoch [300/300] Train Loss: 0.5383 Val Loss: 0.7815\n",
      "Training SGD_gamma0.1_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.6566 Val Loss: 0.8030\n",
      "Epoch [100/300] Train Loss: 0.6999 Val Loss: 0.7925\n",
      "Epoch [150/300] Train Loss: 0.6911 Val Loss: 0.7923\n",
      "Epoch [200/300] Train Loss: 0.8394 Val Loss: 0.7923\n",
      "Epoch [250/300] Train Loss: 0.8732 Val Loss: 0.7923\n",
      "Epoch [300/300] Train Loss: 0.7577 Val Loss: 0.7923\n",
      "Training SGD_gamma0.1_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.6612 Val Loss: 0.7445\n",
      "Epoch [100/300] Train Loss: 0.8676 Val Loss: 0.7326\n",
      "Epoch [150/300] Train Loss: 0.5156 Val Loss: 0.7324\n",
      "Epoch [200/300] Train Loss: 0.4988 Val Loss: 0.7324\n",
      "Epoch [250/300] Train Loss: 1.1131 Val Loss: 0.7324\n",
      "Epoch [300/300] Train Loss: 0.8642 Val Loss: 0.7324\n",
      "Training SGD_gamma0.1_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 4.6620 Val Loss: 5.6224\n",
      "Epoch [100/300] Train Loss: 4.3825 Val Loss: 4.5416\n",
      "Epoch [150/300] Train Loss: 4.5714 Val Loss: 4.5250\n",
      "Epoch [200/300] Train Loss: 3.8536 Val Loss: 4.5250\n",
      "Epoch [250/300] Train Loss: 4.1397 Val Loss: 4.5250\n",
      "Epoch [300/300] Train Loss: 3.8081 Val Loss: 4.5250\n",
      "Training SGD_gamma0.1_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 7.8717 Val Loss: 7.3097\n",
      "Epoch [100/300] Train Loss: 5.0775 Val Loss: 5.9337\n",
      "Epoch [150/300] Train Loss: 6.8064 Val Loss: 5.9119\n",
      "Epoch [200/300] Train Loss: 5.8764 Val Loss: 5.9119\n",
      "Epoch [250/300] Train Loss: 6.0305 Val Loss: 5.9119\n",
      "Epoch [300/300] Train Loss: 5.3251 Val Loss: 5.9119\n",
      "Training SGD_gamma0.1_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 7.1846 Val Loss: 6.7251\n",
      "Epoch [100/300] Train Loss: 5.8226 Val Loss: 5.4397\n",
      "Epoch [150/300] Train Loss: 5.4618 Val Loss: 5.4193\n",
      "Epoch [200/300] Train Loss: 5.5514 Val Loss: 5.4193\n",
      "Epoch [250/300] Train Loss: 5.1527 Val Loss: 5.4193\n",
      "Epoch [300/300] Train Loss: 5.2435 Val Loss: 5.4193\n",
      "Training SGD_gamma0.001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4860 Val Loss: 0.5008\n",
      "Epoch [100/300] Train Loss: 0.4951 Val Loss: 0.5000\n",
      "Epoch [150/300] Train Loss: 0.4819 Val Loss: 0.5000\n",
      "Epoch [200/300] Train Loss: 0.4231 Val Loss: 0.5000\n",
      "Epoch [250/300] Train Loss: 0.5296 Val Loss: 0.5000\n",
      "Epoch [300/300] Train Loss: 0.6225 Val Loss: 0.5000\n",
      "Training SGD_gamma0.001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.4248 Val Loss: 0.5075\n",
      "Epoch [100/300] Train Loss: 0.3384 Val Loss: 0.5073\n",
      "Epoch [150/300] Train Loss: 0.4803 Val Loss: 0.5073\n",
      "Epoch [200/300] Train Loss: 0.5325 Val Loss: 0.5073\n",
      "Epoch [250/300] Train Loss: 0.4437 Val Loss: 0.5073\n",
      "Epoch [300/300] Train Loss: 0.4231 Val Loss: 0.5073\n",
      "Training SGD_gamma0.001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5109 Val Loss: 0.5057\n",
      "Epoch [100/300] Train Loss: 0.2743 Val Loss: 0.5049\n",
      "Epoch [150/300] Train Loss: 0.5118 Val Loss: 0.5049\n",
      "Epoch [200/300] Train Loss: 0.4103 Val Loss: 0.5049\n",
      "Epoch [250/300] Train Loss: 0.4850 Val Loss: 0.5049\n",
      "Epoch [300/300] Train Loss: 0.4431 Val Loss: 0.5049\n",
      "Training SGD_gamma0.001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.6774 Val Loss: 0.8364\n",
      "Epoch [100/300] Train Loss: 1.1312 Val Loss: 0.8363\n",
      "Epoch [150/300] Train Loss: 1.0944 Val Loss: 0.8363\n",
      "Epoch [200/300] Train Loss: 0.5037 Val Loss: 0.8363\n",
      "Epoch [250/300] Train Loss: 0.8766 Val Loss: 0.8363\n",
      "Epoch [300/300] Train Loss: 0.6622 Val Loss: 0.8363\n",
      "Training SGD_gamma0.001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.8058 Val Loss: 0.8259\n",
      "Epoch [100/300] Train Loss: 0.7774 Val Loss: 0.8258\n",
      "Epoch [150/300] Train Loss: 1.1748 Val Loss: 0.8258\n",
      "Epoch [200/300] Train Loss: 0.7683 Val Loss: 0.8258\n",
      "Epoch [250/300] Train Loss: 0.7018 Val Loss: 0.8258\n",
      "Epoch [300/300] Train Loss: 1.1398 Val Loss: 0.8258\n",
      "Training SGD_gamma0.001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.9895 Val Loss: 0.9420\n",
      "Epoch [100/300] Train Loss: 0.9041 Val Loss: 0.9418\n",
      "Epoch [150/300] Train Loss: 0.9611 Val Loss: 0.9418\n",
      "Epoch [200/300] Train Loss: 0.9646 Val Loss: 0.9418\n",
      "Epoch [250/300] Train Loss: 0.5345 Val Loss: 0.9418\n",
      "Epoch [300/300] Train Loss: 0.6916 Val Loss: 0.9418\n",
      "Training SGD_gamma0.001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 6.1520 Val Loss: 6.9135\n",
      "Epoch [100/300] Train Loss: 7.4109 Val Loss: 6.9038\n",
      "Epoch [150/300] Train Loss: 7.3018 Val Loss: 6.9038\n",
      "Epoch [200/300] Train Loss: 6.8416 Val Loss: 6.9038\n",
      "Epoch [250/300] Train Loss: 8.0536 Val Loss: 6.9038\n",
      "Epoch [300/300] Train Loss: 7.0157 Val Loss: 6.9038\n",
      "Training SGD_gamma0.001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 16.6735 Val Loss: 14.7262\n",
      "Epoch [100/300] Train Loss: 15.9041 Val Loss: 14.7158\n",
      "Epoch [150/300] Train Loss: 15.1053 Val Loss: 14.7158\n",
      "Epoch [200/300] Train Loss: 13.8159 Val Loss: 14.7158\n",
      "Epoch [250/300] Train Loss: 11.9991 Val Loss: 14.7158\n",
      "Epoch [300/300] Train Loss: 14.8617 Val Loss: 14.7158\n",
      "Training SGD_gamma0.001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 11.6367 Val Loss: 11.2819\n",
      "Epoch [100/300] Train Loss: 10.9138 Val Loss: 11.2704\n",
      "Epoch [150/300] Train Loss: 10.9144 Val Loss: 11.2704\n",
      "Epoch [200/300] Train Loss: 11.3171 Val Loss: 11.2704\n",
      "Epoch [250/300] Train Loss: 11.4380 Val Loss: 11.2704\n",
      "Epoch [300/300] Train Loss: 12.0501 Val Loss: 11.2704\n",
      "Training SGD_gamma0.0001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4598 Val Loss: 0.5077\n",
      "Epoch [100/300] Train Loss: 0.3917 Val Loss: 0.5077\n",
      "Epoch [150/300] Train Loss: 0.3205 Val Loss: 0.5077\n",
      "Epoch [200/300] Train Loss: 0.4710 Val Loss: 0.5077\n",
      "Epoch [250/300] Train Loss: 0.4450 Val Loss: 0.5077\n",
      "Epoch [300/300] Train Loss: 0.3345 Val Loss: 0.5077\n",
      "Training SGD_gamma0.0001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.6345 Val Loss: 0.4807\n",
      "Epoch [100/300] Train Loss: 0.7039 Val Loss: 0.4805\n",
      "Epoch [150/300] Train Loss: 0.5687 Val Loss: 0.4805\n",
      "Epoch [200/300] Train Loss: 0.4474 Val Loss: 0.4805\n",
      "Epoch [250/300] Train Loss: 0.4819 Val Loss: 0.4805\n",
      "Epoch [300/300] Train Loss: 0.5277 Val Loss: 0.4805\n",
      "Training SGD_gamma0.0001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5091 Val Loss: 0.5083\n",
      "Epoch [100/300] Train Loss: 0.7005 Val Loss: 0.5082\n",
      "Epoch [150/300] Train Loss: 0.5451 Val Loss: 0.5082\n",
      "Epoch [200/300] Train Loss: 0.4809 Val Loss: 0.5082\n",
      "Epoch [250/300] Train Loss: 0.7115 Val Loss: 0.5082\n",
      "Epoch [300/300] Train Loss: 0.5271 Val Loss: 0.5082\n",
      "Training SGD_gamma0.0001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.7802 Val Loss: 0.9051\n",
      "Epoch [100/300] Train Loss: 1.1430 Val Loss: 0.9051\n",
      "Epoch [150/300] Train Loss: 0.7176 Val Loss: 0.9051\n",
      "Epoch [200/300] Train Loss: 0.5650 Val Loss: 0.9051\n",
      "Epoch [250/300] Train Loss: 0.9316 Val Loss: 0.9051\n",
      "Epoch [300/300] Train Loss: 1.3620 Val Loss: 0.9051\n",
      "Training SGD_gamma0.0001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.7663 Val Loss: 0.7685\n",
      "Epoch [100/300] Train Loss: 0.8448 Val Loss: 0.7685\n",
      "Epoch [150/300] Train Loss: 0.8207 Val Loss: 0.7685\n",
      "Epoch [200/300] Train Loss: 0.5538 Val Loss: 0.7685\n",
      "Epoch [250/300] Train Loss: 0.5516 Val Loss: 0.7685\n",
      "Epoch [300/300] Train Loss: 0.5988 Val Loss: 0.7685\n",
      "Training SGD_gamma0.0001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.7123 Val Loss: 0.8635\n",
      "Epoch [100/300] Train Loss: 0.7733 Val Loss: 0.8634\n",
      "Epoch [150/300] Train Loss: 0.8460 Val Loss: 0.8634\n",
      "Epoch [200/300] Train Loss: 0.8891 Val Loss: 0.8634\n",
      "Epoch [250/300] Train Loss: 0.9946 Val Loss: 0.8634\n",
      "Epoch [300/300] Train Loss: 0.5908 Val Loss: 0.8634\n",
      "Training SGD_gamma0.0001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 8.1209 Val Loss: 8.3659\n",
      "Epoch [100/300] Train Loss: 8.0010 Val Loss: 8.3648\n",
      "Epoch [150/300] Train Loss: 7.8379 Val Loss: 8.3648\n",
      "Epoch [200/300] Train Loss: 8.0259 Val Loss: 8.3648\n",
      "Epoch [250/300] Train Loss: 7.7394 Val Loss: 8.3648\n",
      "Epoch [300/300] Train Loss: 8.7425 Val Loss: 8.3648\n",
      "Training SGD_gamma0.0001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 7.6446 Val Loss: 7.7871\n",
      "Epoch [100/300] Train Loss: 8.1475 Val Loss: 7.7862\n",
      "Epoch [150/300] Train Loss: 8.2573 Val Loss: 7.7862\n",
      "Epoch [200/300] Train Loss: 8.8800 Val Loss: 7.7862\n",
      "Epoch [250/300] Train Loss: 7.6964 Val Loss: 7.7862\n",
      "Epoch [300/300] Train Loss: 7.4776 Val Loss: 7.7862\n",
      "Training SGD_gamma0.0001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 4.3905 Val Loss: 4.9870\n",
      "Epoch [100/300] Train Loss: 6.3455 Val Loss: 4.9863\n",
      "Epoch [150/300] Train Loss: 4.6191 Val Loss: 4.9863\n",
      "Epoch [200/300] Train Loss: 4.0143 Val Loss: 4.9863\n",
      "Epoch [250/300] Train Loss: 4.7472 Val Loss: 4.9863\n",
      "Epoch [300/300] Train Loss: 4.3814 Val Loss: 4.9863\n",
      "Training Adam_gamma0.1_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4260 Val Loss: 0.4880\n",
      "Epoch [100/300] Train Loss: 0.3756 Val Loss: 0.4883\n",
      "Epoch [150/300] Train Loss: 0.4144 Val Loss: 0.4885\n",
      "Epoch [200/300] Train Loss: 0.2612 Val Loss: 0.4885\n",
      "Epoch [250/300] Train Loss: 0.1770 Val Loss: 0.4885\n",
      "Epoch [300/300] Train Loss: 0.4847 Val Loss: 0.4885\n",
      "Training Adam_gamma0.1_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.4705 Val Loss: 0.4774\n",
      "Epoch [100/300] Train Loss: 0.3892 Val Loss: 0.4815\n",
      "Epoch [150/300] Train Loss: 0.3374 Val Loss: 0.4816\n",
      "Epoch [200/300] Train Loss: 0.4437 Val Loss: 0.4816\n",
      "Epoch [250/300] Train Loss: 0.2852 Val Loss: 0.4816\n",
      "Epoch [300/300] Train Loss: 0.3074 Val Loss: 0.4816\n",
      "Training Adam_gamma0.1_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.3652 Val Loss: 0.4710\n",
      "Epoch [100/300] Train Loss: 0.2805 Val Loss: 0.4704\n",
      "Epoch [150/300] Train Loss: 0.3655 Val Loss: 0.4704\n",
      "Epoch [200/300] Train Loss: 0.3807 Val Loss: 0.4704\n",
      "Epoch [250/300] Train Loss: 0.4316 Val Loss: 0.4704\n",
      "Epoch [300/300] Train Loss: 0.5266 Val Loss: 0.4704\n",
      "Training Adam_gamma0.1_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4411 Val Loss: 0.5029\n",
      "Epoch [100/300] Train Loss: 0.5044 Val Loss: 0.5022\n",
      "Epoch [150/300] Train Loss: 0.2711 Val Loss: 0.5022\n",
      "Epoch [200/300] Train Loss: 0.6012 Val Loss: 0.5022\n",
      "Epoch [250/300] Train Loss: 0.3326 Val Loss: 0.5022\n",
      "Epoch [300/300] Train Loss: 0.3969 Val Loss: 0.5022\n",
      "Training Adam_gamma0.1_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.3823 Val Loss: 0.4918\n",
      "Epoch [100/300] Train Loss: 0.5136 Val Loss: 0.4897\n",
      "Epoch [150/300] Train Loss: 0.4900 Val Loss: 0.4897\n",
      "Epoch [200/300] Train Loss: 0.4358 Val Loss: 0.4897\n",
      "Epoch [250/300] Train Loss: 0.3858 Val Loss: 0.4897\n",
      "Epoch [300/300] Train Loss: 0.3904 Val Loss: 0.4897\n",
      "Training Adam_gamma0.1_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.4480 Val Loss: 0.4969\n",
      "Epoch [100/300] Train Loss: 0.6345 Val Loss: 0.4950\n",
      "Epoch [150/300] Train Loss: 0.5950 Val Loss: 0.4951\n",
      "Epoch [200/300] Train Loss: 0.5845 Val Loss: 0.4951\n",
      "Epoch [250/300] Train Loss: 0.4171 Val Loss: 0.4951\n",
      "Epoch [300/300] Train Loss: 0.6201 Val Loss: 0.4951\n",
      "Training Adam_gamma0.1_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 1.4231 Val Loss: 2.1560\n",
      "Epoch [100/300] Train Loss: 1.5671 Val Loss: 2.0955\n",
      "Epoch [150/300] Train Loss: 1.5766 Val Loss: 2.0942\n",
      "Epoch [200/300] Train Loss: 2.1457 Val Loss: 2.0942\n",
      "Epoch [250/300] Train Loss: 2.2701 Val Loss: 2.0942\n",
      "Epoch [300/300] Train Loss: 1.7825 Val Loss: 2.0942\n",
      "Training Adam_gamma0.1_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 1.7048 Val Loss: 2.2348\n",
      "Epoch [100/300] Train Loss: 1.8500 Val Loss: 2.1372\n",
      "Epoch [150/300] Train Loss: 1.5111 Val Loss: 2.1353\n",
      "Epoch [200/300] Train Loss: 1.4540 Val Loss: 2.1353\n",
      "Epoch [250/300] Train Loss: 1.9005 Val Loss: 2.1353\n",
      "Epoch [300/300] Train Loss: 1.7878 Val Loss: 2.1353\n",
      "Training Adam_gamma0.1_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 1.1237 Val Loss: 1.7271\n",
      "Epoch [100/300] Train Loss: 1.3537 Val Loss: 1.6600\n",
      "Epoch [150/300] Train Loss: 1.4461 Val Loss: 1.6586\n",
      "Epoch [200/300] Train Loss: 1.3345 Val Loss: 1.6586\n",
      "Epoch [250/300] Train Loss: 1.9069 Val Loss: 1.6586\n",
      "Epoch [300/300] Train Loss: 1.7127 Val Loss: 1.6586\n",
      "Training Adam_gamma0.001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4160 Val Loss: 0.4923\n",
      "Epoch [100/300] Train Loss: 0.3462 Val Loss: 0.4879\n",
      "Epoch [150/300] Train Loss: 0.4845 Val Loss: 0.4879\n",
      "Epoch [200/300] Train Loss: 0.5644 Val Loss: 0.4879\n",
      "Epoch [250/300] Train Loss: 0.4178 Val Loss: 0.4879\n",
      "Epoch [300/300] Train Loss: 0.3721 Val Loss: 0.4879\n",
      "Training Adam_gamma0.001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.3260 Val Loss: 0.4739\n",
      "Epoch [100/300] Train Loss: 0.2880 Val Loss: 0.4729\n",
      "Epoch [150/300] Train Loss: 0.3697 Val Loss: 0.4729\n",
      "Epoch [200/300] Train Loss: 0.3148 Val Loss: 0.4729\n",
      "Epoch [250/300] Train Loss: 0.3497 Val Loss: 0.4729\n",
      "Epoch [300/300] Train Loss: 0.3033 Val Loss: 0.4729\n",
      "Training Adam_gamma0.001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.3862 Val Loss: 0.4844\n",
      "Epoch [100/300] Train Loss: 0.2765 Val Loss: 0.4823\n",
      "Epoch [150/300] Train Loss: 0.3012 Val Loss: 0.4823\n",
      "Epoch [200/300] Train Loss: 0.3864 Val Loss: 0.4823\n",
      "Epoch [250/300] Train Loss: 0.3531 Val Loss: 0.4823\n",
      "Epoch [300/300] Train Loss: 0.2992 Val Loss: 0.4823\n",
      "Training Adam_gamma0.001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4217 Val Loss: 0.4937\n",
      "Epoch [100/300] Train Loss: 0.5161 Val Loss: 0.4935\n",
      "Epoch [150/300] Train Loss: 0.5919 Val Loss: 0.4935\n",
      "Epoch [200/300] Train Loss: 0.3973 Val Loss: 0.4935\n",
      "Epoch [250/300] Train Loss: 0.4102 Val Loss: 0.4935\n",
      "Epoch [300/300] Train Loss: 0.4448 Val Loss: 0.4935\n",
      "Training Adam_gamma0.001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.5457 Val Loss: 0.5091\n",
      "Epoch [100/300] Train Loss: 0.3926 Val Loss: 0.5093\n",
      "Epoch [150/300] Train Loss: 0.5253 Val Loss: 0.5093\n",
      "Epoch [200/300] Train Loss: 0.4494 Val Loss: 0.5093\n",
      "Epoch [250/300] Train Loss: 0.5432 Val Loss: 0.5093\n",
      "Epoch [300/300] Train Loss: 0.4574 Val Loss: 0.5093\n",
      "Training Adam_gamma0.001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.4589 Val Loss: 0.5112\n",
      "Epoch [100/300] Train Loss: 0.4663 Val Loss: 0.5110\n",
      "Epoch [150/300] Train Loss: 0.4374 Val Loss: 0.5110\n",
      "Epoch [200/300] Train Loss: 0.4485 Val Loss: 0.5110\n",
      "Epoch [250/300] Train Loss: 0.3966 Val Loss: 0.5110\n",
      "Epoch [300/300] Train Loss: 0.4276 Val Loss: 0.5110\n",
      "Training Adam_gamma0.001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 1.2964 Val Loss: 2.3401\n",
      "Epoch [100/300] Train Loss: 5.7020 Val Loss: 2.3396\n",
      "Epoch [150/300] Train Loss: 1.7653 Val Loss: 2.3396\n",
      "Epoch [200/300] Train Loss: 1.9588 Val Loss: 2.3396\n",
      "Epoch [250/300] Train Loss: 1.7287 Val Loss: 2.3396\n",
      "Epoch [300/300] Train Loss: 1.9684 Val Loss: 2.3396\n",
      "Training Adam_gamma0.001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 2.7207 Val Loss: 2.2189\n",
      "Epoch [100/300] Train Loss: 1.6967 Val Loss: 2.2183\n",
      "Epoch [150/300] Train Loss: 2.6019 Val Loss: 2.2183\n",
      "Epoch [200/300] Train Loss: 1.7095 Val Loss: 2.2183\n",
      "Epoch [250/300] Train Loss: 1.8255 Val Loss: 2.2183\n",
      "Epoch [300/300] Train Loss: 2.0414 Val Loss: 2.2183\n",
      "Training Adam_gamma0.001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 2.6087 Val Loss: 2.4114\n",
      "Epoch [100/300] Train Loss: 2.6246 Val Loss: 2.4109\n",
      "Epoch [150/300] Train Loss: 1.3658 Val Loss: 2.4109\n",
      "Epoch [200/300] Train Loss: 1.7024 Val Loss: 2.4109\n",
      "Epoch [250/300] Train Loss: 2.2356 Val Loss: 2.4109\n",
      "Epoch [300/300] Train Loss: 3.3904 Val Loss: 2.4109\n",
      "Training Adam_gamma0.0001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4266 Val Loss: 0.5104\n",
      "Epoch [100/300] Train Loss: 0.3948 Val Loss: 0.5081\n",
      "Epoch [150/300] Train Loss: 0.4159 Val Loss: 0.5081\n",
      "Epoch [200/300] Train Loss: 0.3690 Val Loss: 0.5081\n",
      "Epoch [250/300] Train Loss: 0.4289 Val Loss: 0.5081\n",
      "Epoch [300/300] Train Loss: 0.4959 Val Loss: 0.5081\n",
      "Training Adam_gamma0.0001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.3812 Val Loss: 0.4908\n",
      "Epoch [100/300] Train Loss: 0.4760 Val Loss: 0.4901\n",
      "Epoch [150/300] Train Loss: 0.5298 Val Loss: 0.4901\n",
      "Epoch [200/300] Train Loss: 0.4384 Val Loss: 0.4901\n",
      "Epoch [250/300] Train Loss: 0.6355 Val Loss: 0.4901\n",
      "Epoch [300/300] Train Loss: 0.3464 Val Loss: 0.4901\n",
      "Training Adam_gamma0.0001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5962 Val Loss: 0.5195\n",
      "Epoch [100/300] Train Loss: 0.4948 Val Loss: 0.5142\n",
      "Epoch [150/300] Train Loss: 0.5515 Val Loss: 0.5142\n",
      "Epoch [200/300] Train Loss: 0.4633 Val Loss: 0.5142\n",
      "Epoch [250/300] Train Loss: 0.4380 Val Loss: 0.5142\n",
      "Epoch [300/300] Train Loss: 0.5051 Val Loss: 0.5142\n",
      "Training Adam_gamma0.0001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.3876 Val Loss: 0.5045\n",
      "Epoch [100/300] Train Loss: 0.3386 Val Loss: 0.5043\n",
      "Epoch [150/300] Train Loss: 0.5450 Val Loss: 0.5043\n",
      "Epoch [200/300] Train Loss: 0.4200 Val Loss: 0.5043\n",
      "Epoch [250/300] Train Loss: 0.4865 Val Loss: 0.5043\n",
      "Epoch [300/300] Train Loss: 0.5314 Val Loss: 0.5043\n",
      "Training Adam_gamma0.0001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.4155 Val Loss: 0.4895\n",
      "Epoch [100/300] Train Loss: 0.4258 Val Loss: 0.4891\n",
      "Epoch [150/300] Train Loss: 0.4975 Val Loss: 0.4891\n",
      "Epoch [200/300] Train Loss: 0.3966 Val Loss: 0.4891\n",
      "Epoch [250/300] Train Loss: 0.4301 Val Loss: 0.4891\n",
      "Epoch [300/300] Train Loss: 0.6642 Val Loss: 0.4891\n",
      "Training Adam_gamma0.0001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5717 Val Loss: 0.5200\n",
      "Epoch [100/300] Train Loss: 0.4058 Val Loss: 0.5194\n",
      "Epoch [150/300] Train Loss: 0.3664 Val Loss: 0.5194\n",
      "Epoch [200/300] Train Loss: 0.3925 Val Loss: 0.5194\n",
      "Epoch [250/300] Train Loss: 0.6854 Val Loss: 0.5194\n",
      "Epoch [300/300] Train Loss: 0.3494 Val Loss: 0.5194\n",
      "Training Adam_gamma0.0001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 1.9227 Val Loss: 2.0974\n",
      "Epoch [100/300] Train Loss: 1.8038 Val Loss: 2.0974\n",
      "Epoch [150/300] Train Loss: 2.3086 Val Loss: 2.0974\n",
      "Epoch [200/300] Train Loss: 1.4666 Val Loss: 2.0974\n",
      "Epoch [250/300] Train Loss: 1.8436 Val Loss: 2.0974\n",
      "Epoch [300/300] Train Loss: 2.4540 Val Loss: 2.0974\n",
      "Training Adam_gamma0.0001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 1.7534 Val Loss: 1.9912\n",
      "Epoch [100/300] Train Loss: 1.7177 Val Loss: 1.9912\n",
      "Epoch [150/300] Train Loss: 1.4168 Val Loss: 1.9912\n",
      "Epoch [200/300] Train Loss: 5.6824 Val Loss: 1.9912\n",
      "Epoch [250/300] Train Loss: 1.2660 Val Loss: 1.9912\n",
      "Epoch [300/300] Train Loss: 1.7322 Val Loss: 1.9912\n",
      "Training Adam_gamma0.0001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 2.1366 Val Loss: 2.1137\n",
      "Epoch [100/300] Train Loss: 1.8508 Val Loss: 2.1136\n",
      "Epoch [150/300] Train Loss: 1.9761 Val Loss: 2.1136\n",
      "Epoch [200/300] Train Loss: 4.9027 Val Loss: 2.1136\n",
      "Epoch [250/300] Train Loss: 1.4694 Val Loss: 2.1136\n",
      "Epoch [300/300] Train Loss: 1.3050 Val Loss: 2.1136\n",
      "SGD_gamma0.1_lr0.01_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.01_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.01_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.0001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.0001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.01_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.01_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.01_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.01_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.01_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.01_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.01_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.0001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.0001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.01_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.01_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.01_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.01_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.0001_batch1600 Test Loss: 0.4749\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define a function for training and evaluating the model with different hyperparameters\n",
    "def train_evaluate(model, optimizer_name, gamma, learning_rate, batch_size, train_loader, val_loader):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=gamma)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 300\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "        train_loss_history.append(loss.item())\n",
    "        val_loss_history.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {loss.item():.4f} Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Define hyperparameter combinations to explore\n",
    "optimizers = ['SGD', 'Adam']\n",
    "gammas = [0.1, 0.001, 0.0001]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "batch_sizes = [16, 160, 1600]\n",
    "\n",
    "# Perform the experiment\n",
    "results = {}\n",
    "best_loss = 100\n",
    "for optimizer_name in optimizers:\n",
    "    for gamma in gammas:\n",
    "        for learning_rate in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                key = f\"{optimizer_name}_gamma{gamma}_lr{learning_rate}_batch{batch_size}\"\n",
    "                print(f\"Training {key}...\")\n",
    "                model = WineQualityRegressor(input_size, hidden_size)\n",
    "                train_loss, val_loss = train_evaluate(model, optimizer_name, gamma, learning_rate, batch_size, train_loader, val_loader)\n",
    "                if val_loss[-1] < best_loss:\n",
    "                    best_loss = val_loss[-1]\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                results[key] = {\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss\n",
    "                }\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_losses = {}\n",
    "for key, result in results.items():\n",
    "    model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "    test_losses[key] = test_loss.item()\n",
    "\n",
    "# Print test losses\n",
    "for key, test_loss in test_losses.items():\n",
    "    print(f\"{key} Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeab678-4a93-4782-ab11-3c178b2a34d9",
   "metadata": {},
   "source": [
    "The combination of Adam_gamma0.1_lr0.01_batch1600 is the best. The model is just right, because the train and validation loss are not too different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b7f3a-9143-44fb-9438-eb37776bde09",
   "metadata": {},
   "source": [
    "![q](pic/q4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c6e77e4-0ee2-4eda-a602-4e6bdd9e3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00fb4dbd-b68e-47cf-92a2-394ca76b0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "m = 784\n",
    "k = 10\n",
    "n_epochs = 30\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1237ee07-965a-4969-97ee-5b60e14722b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "data1 = datasets.MNIST(\"./data\", train = True, transform = transform, download = True)\n",
    "data2 = datasets.MNIST(\"./data\", train = False, transform = transform, download = True)\n",
    "train_data_loader = torch.utils.data.DataLoader(data1, batch_size = batch_size, shuffle = True)\n",
    "test_data_loader = torch.utils.data.DataLoader(data2, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8cbc4fe0-ab2c-4a28-a3e6-23ce638067f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 279.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, test accuracy 0.8831, loss 0.4770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 284.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, test accuracy 0.8948, loss 0.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 274.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, test accuracy 0.9000, loss 0.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 249.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, test accuracy 0.9054, loss 0.3507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 264.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, test accuracy 0.9069, loss 0.3386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 270.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, test accuracy 0.9096, loss 0.3302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 260.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, test accuracy 0.9107, loss 0.3232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 229.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, test accuracy 0.9125, loss 0.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:12<00:00, 146.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, test accuracy 0.9131, loss 0.3139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:19<00:00, 94.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, test accuracy 0.9144, loss 0.3103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:09<00:00, 201.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, test accuracy 0.9165, loss 0.3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:09<00:00, 194.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, test accuracy 0.9155, loss 0.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 253.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, test accuracy 0.9158, loss 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 271.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, test accuracy 0.9168, loss 0.3005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:09<00:00, 194.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, test accuracy 0.9169, loss 0.2995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 259.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, test accuracy 0.9171, loss 0.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 243.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, test accuracy 0.9180, loss 0.2961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 267.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, test accuracy 0.9183, loss 0.2950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 292.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, test accuracy 0.9180, loss 0.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 300.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, test accuracy 0.9188, loss 0.2927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 290.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, test accuracy 0.9187, loss 0.2918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 292.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, test accuracy 0.9190, loss 0.2908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 286.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, test accuracy 0.9191, loss 0.2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 295.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, test accuracy 0.9195, loss 0.2894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 285.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, test accuracy 0.9190, loss 0.2887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 296.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, test accuracy 0.9201, loss 0.2878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 279.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, test accuracy 0.9206, loss 0.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 297.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, test accuracy 0.9200, loss 0.2865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 299.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, test accuracy 0.9204, loss 0.2867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 297.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, test accuracy 0.9199, loss 0.2858\n"
     ]
    }
   ],
   "source": [
    "#  Momentum method with parameter  = 0.9\n",
    "w = torch.randn((m, k), device = device)*1e-2\n",
    "w.requires_grad_()\n",
    "\n",
    "v = torch.zeros((m, k), device = device)\n",
    "\n",
    "beta = 0.9\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "for idx_epoch in range(n_epochs):\n",
    "    for data, target in tqdm(train_data_loader, ncols=80):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "\n",
    "        y_hat = data @ w\n",
    "        loss = criterion(y_hat, target)\n",
    "\n",
    "        dw = grad(loss, w)[0]\n",
    "        v = beta * v - lr * dw\n",
    "        w = w + v\n",
    "        \n",
    "        # print(data.shape, target.shape, w.shape, y_hat.shape, loss.shape, dW.shape)\n",
    "\n",
    "    num_correct, num_all, loss = 0, 0, 0.\n",
    "    for data, target in test_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        y_hat = data @ w\n",
    "        pred_class = torch.argmax(y_hat, dim=1)\n",
    "        num_correct += torch.sum(pred_class == target)\n",
    "        num_all += target.shape[0]\n",
    "        loss += F.cross_entropy(y_hat, target, reduction='sum')\n",
    "    print('Epoch %d, test accuracy %.4f, loss %.4f' % (idx_epoch, num_correct/num_all, loss/num_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9db399a-8e87-46ec-bd0f-cf7f4db4be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1875 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sqrt(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, target)\n\u001b[1;32m     24\u001b[0m dw \u001b[38;5;241m=\u001b[39m grad(loss, w)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m lamb \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     27\u001b[0m lamb_new \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m lamb \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m gama_minus_1 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lamb)\u001b[38;5;241m/\u001b[39mlamb_new\n",
      "\u001b[0;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "#  Nesterovs Accelerated Gradient (NAG) with parameter  = 0.95\n",
    "w = torch.randn((m, k), device = device)*1e-2\n",
    "w.requires_grad_()\n",
    "\n",
    "v = torch.zeros((m, k), device = device)\n",
    "\n",
    "lamb = 0.\n",
    "gama = 0.\n",
    "ye = 0.\n",
    "\n",
    "beta = 0.95\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "for idx_epoch in range(n_epochs):\n",
    "    for data, target in tqdm(train_data_loader, ncols=80):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "\n",
    "        y_hat = data @ w\n",
    "        loss = criterion(y_hat, target)\n",
    "\n",
    "        dw = grad(loss, w)[0]\n",
    "        \n",
    "        lamb = (1 + torch.sqrt(1 + 4 * lamb ** 2))/2\n",
    "        lamb_new = (1 + torch.sqrt(1 + 4 * lamb ** 2))/2\n",
    "        gama_minus_1 = (1 - lamb)/lamb_new\n",
    "        ye = w - 1/beta * dw\n",
    "        \n",
    "        v = beta * v - lr * dw\n",
    "        w = w + v\n",
    "        \n",
    "        # print(data.shape, target.shape, w.shape, y_hat.shape, loss.shape, dW.shape)\n",
    "\n",
    "    num_correct, num_all, loss = 0, 0, 0.\n",
    "    for data, target in test_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        y_hat = data @ w\n",
    "        pred_class = torch.argmax(y_hat, dim=1)\n",
    "        num_correct += torch.sum(pred_class == target)\n",
    "        num_all += target.shape[0]\n",
    "        loss += F.cross_entropy(y_hat, target, reduction='sum')\n",
    "    print('Epoch %d, test accuracy %.4f, loss %.4f' % (idx_epoch, num_correct/num_all, loss/num_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3b954eb-bb46-43ed-ab86-a416af56e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 271.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, test accuracy 0.9141, loss 0.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 252.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, test accuracy 0.9211, loss 0.2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 248.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, test accuracy 0.9222, loss 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 237.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, test accuracy 0.9239, loss 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 250.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, test accuracy 0.9254, loss 0.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 273.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, test accuracy 0.9269, loss 0.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 279.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, test accuracy 0.9256, loss 0.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:09<00:00, 207.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, test accuracy 0.9256, loss 0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:11<00:00, 159.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, test accuracy 0.9266, loss 0.2810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 270.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, test accuracy 0.9251, loss 0.2796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 283.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, test accuracy 0.9264, loss 0.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 274.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, test accuracy 0.9244, loss 0.2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 280.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, test accuracy 0.9270, loss 0.2849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 277.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, test accuracy 0.9256, loss 0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 255.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, test accuracy 0.9257, loss 0.2833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 239.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, test accuracy 0.9253, loss 0.2869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 272.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, test accuracy 0.9263, loss 0.2869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 280.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, test accuracy 0.9245, loss 0.2874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 277.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, test accuracy 0.9264, loss 0.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 275.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, test accuracy 0.9253, loss 0.2867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 273.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, test accuracy 0.9244, loss 0.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 272.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, test accuracy 0.9245, loss 0.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 265.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, test accuracy 0.9263, loss 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 265.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, test accuracy 0.9256, loss 0.2902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 284.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, test accuracy 0.9260, loss 0.2894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 255.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, test accuracy 0.9259, loss 0.2930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 258.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, test accuracy 0.9262, loss 0.2892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 211.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, test accuracy 0.9267, loss 0.2920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 232.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, test accuracy 0.9257, loss 0.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 282.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, test accuracy 0.9271, loss 0.2899\n"
     ]
    }
   ],
   "source": [
    "#  RMSprop with parameters  = 0.95,  = 1 and  = 108\n",
    "w = torch.randn((m, k), device = device)*1e-2\n",
    "w.requires_grad_()\n",
    "\n",
    "r = torch.zeros((m, k), device = device)\n",
    "\n",
    "beta = 0.95\n",
    "epsi = 1e-8\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "for idx_epoch in range(n_epochs):\n",
    "    for data, target in tqdm(train_data_loader, ncols=80):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "\n",
    "        y_hat = data @ w\n",
    "        loss = criterion(y_hat, target)\n",
    "\n",
    "        dw = grad(loss, w)[0]\n",
    "        \n",
    "        r = beta * r + (1 - beta) * dw**2\n",
    "        deta_w = - lr / torch.sqrt(epsi + r) * dw\n",
    "        w = w + deta_w\n",
    "        \n",
    "        # print(data.shape, target.shape, w.shape, y_hat.shape, loss.shape, dW.shape)\n",
    "\n",
    "    num_correct, num_all, loss = 0, 0, 0.\n",
    "    for data, target in test_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        y_hat = data @ w\n",
    "        pred_class = torch.argmax(y_hat, dim=1)\n",
    "        num_correct += torch.sum(pred_class == target)\n",
    "        num_all += target.shape[0]\n",
    "        loss += F.cross_entropy(y_hat, target, reduction='sum')\n",
    "    print('Epoch %d, test accuracy %.4f, loss %.4f' % (idx_epoch, num_correct/num_all, loss/num_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e52de7cf-3244-4e55-9503-ab6559e906f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 267.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, test accuracy 0.9092, loss 0.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 282.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, test accuracy 0.9147, loss 0.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 271.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, test accuracy 0.9174, loss 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 275.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, test accuracy 0.9211, loss 0.2869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 272.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, test accuracy 0.9217, loss 0.2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 248.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, test accuracy 0.9220, loss 0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 285.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, test accuracy 0.9229, loss 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 237.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, test accuracy 0.9241, loss 0.2735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 280.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, test accuracy 0.9245, loss 0.2737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 273.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, test accuracy 0.9246, loss 0.2719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 245.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, test accuracy 0.9255, loss 0.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 262.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, test accuracy 0.9255, loss 0.2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 214.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, test accuracy 0.9250, loss 0.2717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:09<00:00, 187.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, test accuracy 0.9259, loss 0.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 257.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, test accuracy 0.9253, loss 0.2689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 265.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, test accuracy 0.9259, loss 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 275.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, test accuracy 0.9258, loss 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 276.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, test accuracy 0.9254, loss 0.2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 241.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, test accuracy 0.9260, loss 0.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 248.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, test accuracy 0.9267, loss 0.2683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 273.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, test accuracy 0.9265, loss 0.2681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 275.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, test accuracy 0.9275, loss 0.2671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 229.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, test accuracy 0.9269, loss 0.2678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 221.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, test accuracy 0.9257, loss 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 224.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, test accuracy 0.9277, loss 0.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:06<00:00, 268.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, test accuracy 0.9266, loss 0.2674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 265.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, test accuracy 0.9275, loss 0.2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:08<00:00, 225.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, test accuracy 0.9268, loss 0.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:07<00:00, 246.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, test accuracy 0.9271, loss 0.2672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1875/1875 [00:09<00:00, 202.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, test accuracy 0.9272, loss 0.2670\n"
     ]
    }
   ],
   "source": [
    "#  Adam with parameters 1 = 0.9, 2 = 0.999, and  = 108\n",
    "w = torch.randn((m, k), device = device)*1e-2\n",
    "w.requires_grad_()\n",
    "\n",
    "r = torch.zeros((m, k), device = device)\n",
    "s = torch.zeros((m, k), device = device)\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsi = 1e-8\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "for idx_epoch in range(n_epochs):\n",
    "    for data, target in tqdm(train_data_loader, ncols=80):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "\n",
    "        y_hat = data @ w\n",
    "        loss = criterion(y_hat, target)\n",
    "\n",
    "        dw = grad(loss, w)[0]\n",
    "        \n",
    "        s = beta1 * s + (1 - beta1) * dw\n",
    "        r = beta2 * r + (1 - beta2) * dw**2\n",
    "        \n",
    "        s_hat = s/(1 - beta1)\n",
    "        r_hat = r/(1 - beta2)\n",
    "        \n",
    "        deta_w = - lr * s_hat / torch.sqrt(epsi + r_hat)\n",
    "        w = w + deta_w\n",
    "        \n",
    "        # print(data.shape, target.shape, w.shape, y_hat.shape, loss.shape, dW.shape)\n",
    "\n",
    "    num_correct, num_all, loss = 0, 0, 0.\n",
    "    for data, target in test_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        y_hat = data @ w\n",
    "        pred_class = torch.argmax(y_hat, dim=1)\n",
    "        num_correct += torch.sum(pred_class == target)\n",
    "        num_all += target.shape[0]\n",
    "        loss += F.cross_entropy(y_hat, target, reduction='sum')\n",
    "    print('Epoch %d, test accuracy %.4f, loss %.4f' % (idx_epoch, num_correct/num_all, loss/num_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8260ea6-8dfc-4a41-a755-67c5dfcb07f3",
   "metadata": {},
   "source": [
    "![q](pic/q5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad03736-8c31-435f-9c48-1af83235b733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE685D",
   "language": "python",
   "name": "ece685d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
