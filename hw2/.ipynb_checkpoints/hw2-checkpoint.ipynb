{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e07311d-f434-4a5d-8c81-b2804f00421a",
   "metadata": {},
   "source": [
    "![q](pic/q1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112691-a89a-4c1d-bbc4-3dbefcb2e063",
   "metadata": {},
   "source": [
    "![s](pic/q1a_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae88f85-3c2e-470a-96f5-01aad36068dc",
   "metadata": {},
   "source": [
    "![s](pic/q1a_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5033cc-b961-43c7-8a32-e043834d00ea",
   "metadata": {},
   "source": [
    "![s](pic/q1a_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6c5a0-a467-44f7-877a-d242746c3651",
   "metadata": {},
   "source": [
    "![q](pic/q1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e5628-ee0b-476e-af0c-cb963ef3c3fb",
   "metadata": {},
   "source": [
    "![s](pic/q1a_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c00a9-437a-4ef9-86ad-f8a538acf24a",
   "metadata": {},
   "source": [
    "![s](pic/q1a_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fb389-e7ec-4155-bbad-c879cc03c985",
   "metadata": {},
   "source": [
    "![q](pic/q2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678da82-8805-476d-b4ba-a72068fe8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Load the UCI Wine dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "wine_data = pd.read_csv(url, sep=\";\")\n",
    "wine_features = wine_data.drop(columns=[\"quality\"])\n",
    "wine_target = wine_data[\"quality\"]\n",
    "\n",
    "# Standardize features, data pre-processing\n",
    "scaler = StandardScaler() ## ??\n",
    "wine_features = scaler.fit_transform(wine_features) ## ??\n",
    "\n",
    "# Split the data into training, validation, and testing sets (64-16-20 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(wine_features, wine_target, test_size=0.36, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5556, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.FloatTensor(y_val.values).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class WineQualityRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(WineQualityRegressor, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "model = WineQualityRegressor(input_size, hidden_size)\n",
    "\n",
    "# Define custom batch size and learning rate\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "    train_loss_history.append(loss.item())\n",
    "    val_loss_history.append(val_loss.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {loss.item():.4f} Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Comment on the model fit\n",
    "# Analyze the loss curves and evaluate the model's performance on the test set.\n",
    "\n",
    "# It makes sense to treat wine quality as a continuous label because wine quality is typically assessed on a scale,\n",
    "# and regression allows us to predict a numeric value that represents the quality. The regression approach can capture\n",
    "# the nuances in wine quality better than treating it as a discrete classification problem.\n",
    "\n",
    "# To further evaluate the model, you can calculate additional metrics such as Mean Absolute Error (MAE) or Root Mean\n",
    "# Squared Error (RMSE) on the test set to assess the model's accuracy in predicting wine quality.\n",
    "\n",
    "# Example:\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.FloatTensor(X_test))\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    mae = mean_absolute_error(y_test, test_outputs.numpy())\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9a058-88da-4a71-8660-3f9fdea67f52",
   "metadata": {},
   "source": [
    "![q](pic/q3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dd22f79-8409-40a0-95f5-0464245b788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD_gamma0.1_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4499 Val Loss: 0.4958\n",
      "Epoch [100/300] Train Loss: 0.5511 Val Loss: 0.4943\n",
      "Epoch [150/300] Train Loss: 0.3772 Val Loss: 0.4943\n",
      "Epoch [200/300] Train Loss: 0.5118 Val Loss: 0.4943\n",
      "Epoch [250/300] Train Loss: 0.5664 Val Loss: 0.4943\n",
      "Epoch [300/300] Train Loss: 0.4212 Val Loss: 0.4943\n",
      "Training SGD_gamma0.1_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.6702 Val Loss: 0.5022\n",
      "Epoch [100/300] Train Loss: 0.5955 Val Loss: 0.5015\n",
      "Epoch [150/300] Train Loss: 0.4030 Val Loss: 0.5014\n",
      "Epoch [200/300] Train Loss: 0.4666 Val Loss: 0.5014\n",
      "Epoch [250/300] Train Loss: 0.4319 Val Loss: 0.5014\n",
      "Epoch [300/300] Train Loss: 0.4294 Val Loss: 0.5014\n",
      "Training SGD_gamma0.1_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.4555 Val Loss: 0.4870\n",
      "Epoch [100/300] Train Loss: 0.4017 Val Loss: 0.4864\n",
      "Epoch [150/300] Train Loss: 0.5282 Val Loss: 0.4864\n",
      "Epoch [200/300] Train Loss: 0.4618 Val Loss: 0.4864\n",
      "Epoch [250/300] Train Loss: 0.6337 Val Loss: 0.4864\n",
      "Epoch [300/300] Train Loss: 0.4226 Val Loss: 0.4864\n",
      "Training SGD_gamma0.1_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.5769 Val Loss: 0.7944\n",
      "Epoch [100/300] Train Loss: 1.0632 Val Loss: 0.7817\n",
      "Epoch [150/300] Train Loss: 0.4402 Val Loss: 0.7815\n",
      "Epoch [200/300] Train Loss: 0.4685 Val Loss: 0.7815\n",
      "Epoch [250/300] Train Loss: 1.2270 Val Loss: 0.7815\n",
      "Epoch [300/300] Train Loss: 0.5383 Val Loss: 0.7815\n",
      "Training SGD_gamma0.1_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.6566 Val Loss: 0.8030\n",
      "Epoch [100/300] Train Loss: 0.6999 Val Loss: 0.7925\n",
      "Epoch [150/300] Train Loss: 0.6911 Val Loss: 0.7923\n",
      "Epoch [200/300] Train Loss: 0.8394 Val Loss: 0.7923\n",
      "Epoch [250/300] Train Loss: 0.8732 Val Loss: 0.7923\n",
      "Epoch [300/300] Train Loss: 0.7577 Val Loss: 0.7923\n",
      "Training SGD_gamma0.1_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.6612 Val Loss: 0.7445\n",
      "Epoch [100/300] Train Loss: 0.8676 Val Loss: 0.7326\n",
      "Epoch [150/300] Train Loss: 0.5156 Val Loss: 0.7324\n",
      "Epoch [200/300] Train Loss: 0.4988 Val Loss: 0.7324\n",
      "Epoch [250/300] Train Loss: 1.1131 Val Loss: 0.7324\n",
      "Epoch [300/300] Train Loss: 0.8642 Val Loss: 0.7324\n",
      "Training SGD_gamma0.1_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 4.6620 Val Loss: 5.6224\n",
      "Epoch [100/300] Train Loss: 4.3825 Val Loss: 4.5416\n",
      "Epoch [150/300] Train Loss: 4.5714 Val Loss: 4.5250\n",
      "Epoch [200/300] Train Loss: 3.8536 Val Loss: 4.5250\n",
      "Epoch [250/300] Train Loss: 4.1397 Val Loss: 4.5250\n",
      "Epoch [300/300] Train Loss: 3.8081 Val Loss: 4.5250\n",
      "Training SGD_gamma0.1_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 7.8717 Val Loss: 7.3097\n",
      "Epoch [100/300] Train Loss: 5.0775 Val Loss: 5.9337\n",
      "Epoch [150/300] Train Loss: 6.8064 Val Loss: 5.9119\n",
      "Epoch [200/300] Train Loss: 5.8764 Val Loss: 5.9119\n",
      "Epoch [250/300] Train Loss: 6.0305 Val Loss: 5.9119\n",
      "Epoch [300/300] Train Loss: 5.3251 Val Loss: 5.9119\n",
      "Training SGD_gamma0.1_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 7.1846 Val Loss: 6.7251\n",
      "Epoch [100/300] Train Loss: 5.8226 Val Loss: 5.4397\n",
      "Epoch [150/300] Train Loss: 5.4618 Val Loss: 5.4193\n",
      "Epoch [200/300] Train Loss: 5.5514 Val Loss: 5.4193\n",
      "Epoch [250/300] Train Loss: 5.1527 Val Loss: 5.4193\n",
      "Epoch [300/300] Train Loss: 5.2435 Val Loss: 5.4193\n",
      "Training SGD_gamma0.001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4860 Val Loss: 0.5008\n",
      "Epoch [100/300] Train Loss: 0.4951 Val Loss: 0.5000\n",
      "Epoch [150/300] Train Loss: 0.4819 Val Loss: 0.5000\n",
      "Epoch [200/300] Train Loss: 0.4231 Val Loss: 0.5000\n",
      "Epoch [250/300] Train Loss: 0.5296 Val Loss: 0.5000\n",
      "Epoch [300/300] Train Loss: 0.6225 Val Loss: 0.5000\n",
      "Training SGD_gamma0.001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.4248 Val Loss: 0.5075\n",
      "Epoch [100/300] Train Loss: 0.3384 Val Loss: 0.5073\n",
      "Epoch [150/300] Train Loss: 0.4803 Val Loss: 0.5073\n",
      "Epoch [200/300] Train Loss: 0.5325 Val Loss: 0.5073\n",
      "Epoch [250/300] Train Loss: 0.4437 Val Loss: 0.5073\n",
      "Epoch [300/300] Train Loss: 0.4231 Val Loss: 0.5073\n",
      "Training SGD_gamma0.001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5109 Val Loss: 0.5057\n",
      "Epoch [100/300] Train Loss: 0.2743 Val Loss: 0.5049\n",
      "Epoch [150/300] Train Loss: 0.5118 Val Loss: 0.5049\n",
      "Epoch [200/300] Train Loss: 0.4103 Val Loss: 0.5049\n",
      "Epoch [250/300] Train Loss: 0.4850 Val Loss: 0.5049\n",
      "Epoch [300/300] Train Loss: 0.4431 Val Loss: 0.5049\n",
      "Training SGD_gamma0.001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.6774 Val Loss: 0.8364\n",
      "Epoch [100/300] Train Loss: 1.1312 Val Loss: 0.8363\n",
      "Epoch [150/300] Train Loss: 1.0944 Val Loss: 0.8363\n",
      "Epoch [200/300] Train Loss: 0.5037 Val Loss: 0.8363\n",
      "Epoch [250/300] Train Loss: 0.8766 Val Loss: 0.8363\n",
      "Epoch [300/300] Train Loss: 0.6622 Val Loss: 0.8363\n",
      "Training SGD_gamma0.001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.8058 Val Loss: 0.8259\n",
      "Epoch [100/300] Train Loss: 0.7774 Val Loss: 0.8258\n",
      "Epoch [150/300] Train Loss: 1.1748 Val Loss: 0.8258\n",
      "Epoch [200/300] Train Loss: 0.7683 Val Loss: 0.8258\n",
      "Epoch [250/300] Train Loss: 0.7018 Val Loss: 0.8258\n",
      "Epoch [300/300] Train Loss: 1.1398 Val Loss: 0.8258\n",
      "Training SGD_gamma0.001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.9895 Val Loss: 0.9420\n",
      "Epoch [100/300] Train Loss: 0.9041 Val Loss: 0.9418\n",
      "Epoch [150/300] Train Loss: 0.9611 Val Loss: 0.9418\n",
      "Epoch [200/300] Train Loss: 0.9646 Val Loss: 0.9418\n",
      "Epoch [250/300] Train Loss: 0.5345 Val Loss: 0.9418\n",
      "Epoch [300/300] Train Loss: 0.6916 Val Loss: 0.9418\n",
      "Training SGD_gamma0.001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 6.1520 Val Loss: 6.9135\n",
      "Epoch [100/300] Train Loss: 7.4109 Val Loss: 6.9038\n",
      "Epoch [150/300] Train Loss: 7.3018 Val Loss: 6.9038\n",
      "Epoch [200/300] Train Loss: 6.8416 Val Loss: 6.9038\n",
      "Epoch [250/300] Train Loss: 8.0536 Val Loss: 6.9038\n",
      "Epoch [300/300] Train Loss: 7.0157 Val Loss: 6.9038\n",
      "Training SGD_gamma0.001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 16.6735 Val Loss: 14.7262\n",
      "Epoch [100/300] Train Loss: 15.9041 Val Loss: 14.7158\n",
      "Epoch [150/300] Train Loss: 15.1053 Val Loss: 14.7158\n",
      "Epoch [200/300] Train Loss: 13.8159 Val Loss: 14.7158\n",
      "Epoch [250/300] Train Loss: 11.9991 Val Loss: 14.7158\n",
      "Epoch [300/300] Train Loss: 14.8617 Val Loss: 14.7158\n",
      "Training SGD_gamma0.001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 11.6367 Val Loss: 11.2819\n",
      "Epoch [100/300] Train Loss: 10.9138 Val Loss: 11.2704\n",
      "Epoch [150/300] Train Loss: 10.9144 Val Loss: 11.2704\n",
      "Epoch [200/300] Train Loss: 11.3171 Val Loss: 11.2704\n",
      "Epoch [250/300] Train Loss: 11.4380 Val Loss: 11.2704\n",
      "Epoch [300/300] Train Loss: 12.0501 Val Loss: 11.2704\n",
      "Training SGD_gamma0.0001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4598 Val Loss: 0.5077\n",
      "Epoch [100/300] Train Loss: 0.3917 Val Loss: 0.5077\n",
      "Epoch [150/300] Train Loss: 0.3205 Val Loss: 0.5077\n",
      "Epoch [200/300] Train Loss: 0.4710 Val Loss: 0.5077\n",
      "Epoch [250/300] Train Loss: 0.4450 Val Loss: 0.5077\n",
      "Epoch [300/300] Train Loss: 0.3345 Val Loss: 0.5077\n",
      "Training SGD_gamma0.0001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.6345 Val Loss: 0.4807\n",
      "Epoch [100/300] Train Loss: 0.7039 Val Loss: 0.4805\n",
      "Epoch [150/300] Train Loss: 0.5687 Val Loss: 0.4805\n",
      "Epoch [200/300] Train Loss: 0.4474 Val Loss: 0.4805\n",
      "Epoch [250/300] Train Loss: 0.4819 Val Loss: 0.4805\n",
      "Epoch [300/300] Train Loss: 0.5277 Val Loss: 0.4805\n",
      "Training SGD_gamma0.0001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5091 Val Loss: 0.5083\n",
      "Epoch [100/300] Train Loss: 0.7005 Val Loss: 0.5082\n",
      "Epoch [150/300] Train Loss: 0.5451 Val Loss: 0.5082\n",
      "Epoch [200/300] Train Loss: 0.4809 Val Loss: 0.5082\n",
      "Epoch [250/300] Train Loss: 0.7115 Val Loss: 0.5082\n",
      "Epoch [300/300] Train Loss: 0.5271 Val Loss: 0.5082\n",
      "Training SGD_gamma0.0001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.7802 Val Loss: 0.9051\n",
      "Epoch [100/300] Train Loss: 1.1430 Val Loss: 0.9051\n",
      "Epoch [150/300] Train Loss: 0.7176 Val Loss: 0.9051\n",
      "Epoch [200/300] Train Loss: 0.5650 Val Loss: 0.9051\n",
      "Epoch [250/300] Train Loss: 0.9316 Val Loss: 0.9051\n",
      "Epoch [300/300] Train Loss: 1.3620 Val Loss: 0.9051\n",
      "Training SGD_gamma0.0001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.7663 Val Loss: 0.7685\n",
      "Epoch [100/300] Train Loss: 0.8448 Val Loss: 0.7685\n",
      "Epoch [150/300] Train Loss: 0.8207 Val Loss: 0.7685\n",
      "Epoch [200/300] Train Loss: 0.5538 Val Loss: 0.7685\n",
      "Epoch [250/300] Train Loss: 0.5516 Val Loss: 0.7685\n",
      "Epoch [300/300] Train Loss: 0.5988 Val Loss: 0.7685\n",
      "Training SGD_gamma0.0001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.7123 Val Loss: 0.8635\n",
      "Epoch [100/300] Train Loss: 0.7733 Val Loss: 0.8634\n",
      "Epoch [150/300] Train Loss: 0.8460 Val Loss: 0.8634\n",
      "Epoch [200/300] Train Loss: 0.8891 Val Loss: 0.8634\n",
      "Epoch [250/300] Train Loss: 0.9946 Val Loss: 0.8634\n",
      "Epoch [300/300] Train Loss: 0.5908 Val Loss: 0.8634\n",
      "Training SGD_gamma0.0001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 8.1209 Val Loss: 8.3659\n",
      "Epoch [100/300] Train Loss: 8.0010 Val Loss: 8.3648\n",
      "Epoch [150/300] Train Loss: 7.8379 Val Loss: 8.3648\n",
      "Epoch [200/300] Train Loss: 8.0259 Val Loss: 8.3648\n",
      "Epoch [250/300] Train Loss: 7.7394 Val Loss: 8.3648\n",
      "Epoch [300/300] Train Loss: 8.7425 Val Loss: 8.3648\n",
      "Training SGD_gamma0.0001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 7.6446 Val Loss: 7.7871\n",
      "Epoch [100/300] Train Loss: 8.1475 Val Loss: 7.7862\n",
      "Epoch [150/300] Train Loss: 8.2573 Val Loss: 7.7862\n",
      "Epoch [200/300] Train Loss: 8.8800 Val Loss: 7.7862\n",
      "Epoch [250/300] Train Loss: 7.6964 Val Loss: 7.7862\n",
      "Epoch [300/300] Train Loss: 7.4776 Val Loss: 7.7862\n",
      "Training SGD_gamma0.0001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 4.3905 Val Loss: 4.9870\n",
      "Epoch [100/300] Train Loss: 6.3455 Val Loss: 4.9863\n",
      "Epoch [150/300] Train Loss: 4.6191 Val Loss: 4.9863\n",
      "Epoch [200/300] Train Loss: 4.0143 Val Loss: 4.9863\n",
      "Epoch [250/300] Train Loss: 4.7472 Val Loss: 4.9863\n",
      "Epoch [300/300] Train Loss: 4.3814 Val Loss: 4.9863\n",
      "Training Adam_gamma0.1_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4260 Val Loss: 0.4880\n",
      "Epoch [100/300] Train Loss: 0.3756 Val Loss: 0.4883\n",
      "Epoch [150/300] Train Loss: 0.4144 Val Loss: 0.4885\n",
      "Epoch [200/300] Train Loss: 0.2612 Val Loss: 0.4885\n",
      "Epoch [250/300] Train Loss: 0.1770 Val Loss: 0.4885\n",
      "Epoch [300/300] Train Loss: 0.4847 Val Loss: 0.4885\n",
      "Training Adam_gamma0.1_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.4705 Val Loss: 0.4774\n",
      "Epoch [100/300] Train Loss: 0.3892 Val Loss: 0.4815\n",
      "Epoch [150/300] Train Loss: 0.3374 Val Loss: 0.4816\n",
      "Epoch [200/300] Train Loss: 0.4437 Val Loss: 0.4816\n",
      "Epoch [250/300] Train Loss: 0.2852 Val Loss: 0.4816\n",
      "Epoch [300/300] Train Loss: 0.3074 Val Loss: 0.4816\n",
      "Training Adam_gamma0.1_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.3652 Val Loss: 0.4710\n",
      "Epoch [100/300] Train Loss: 0.2805 Val Loss: 0.4704\n",
      "Epoch [150/300] Train Loss: 0.3655 Val Loss: 0.4704\n",
      "Epoch [200/300] Train Loss: 0.3807 Val Loss: 0.4704\n",
      "Epoch [250/300] Train Loss: 0.4316 Val Loss: 0.4704\n",
      "Epoch [300/300] Train Loss: 0.5266 Val Loss: 0.4704\n",
      "Training Adam_gamma0.1_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4411 Val Loss: 0.5029\n",
      "Epoch [100/300] Train Loss: 0.5044 Val Loss: 0.5022\n",
      "Epoch [150/300] Train Loss: 0.2711 Val Loss: 0.5022\n",
      "Epoch [200/300] Train Loss: 0.6012 Val Loss: 0.5022\n",
      "Epoch [250/300] Train Loss: 0.3326 Val Loss: 0.5022\n",
      "Epoch [300/300] Train Loss: 0.3969 Val Loss: 0.5022\n",
      "Training Adam_gamma0.1_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.3823 Val Loss: 0.4918\n",
      "Epoch [100/300] Train Loss: 0.5136 Val Loss: 0.4897\n",
      "Epoch [150/300] Train Loss: 0.4900 Val Loss: 0.4897\n",
      "Epoch [200/300] Train Loss: 0.4358 Val Loss: 0.4897\n",
      "Epoch [250/300] Train Loss: 0.3858 Val Loss: 0.4897\n",
      "Epoch [300/300] Train Loss: 0.3904 Val Loss: 0.4897\n",
      "Training Adam_gamma0.1_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.4480 Val Loss: 0.4969\n",
      "Epoch [100/300] Train Loss: 0.6345 Val Loss: 0.4950\n",
      "Epoch [150/300] Train Loss: 0.5950 Val Loss: 0.4951\n",
      "Epoch [200/300] Train Loss: 0.5845 Val Loss: 0.4951\n",
      "Epoch [250/300] Train Loss: 0.4171 Val Loss: 0.4951\n",
      "Epoch [300/300] Train Loss: 0.6201 Val Loss: 0.4951\n",
      "Training Adam_gamma0.1_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 1.4231 Val Loss: 2.1560\n",
      "Epoch [100/300] Train Loss: 1.5671 Val Loss: 2.0955\n",
      "Epoch [150/300] Train Loss: 1.5766 Val Loss: 2.0942\n",
      "Epoch [200/300] Train Loss: 2.1457 Val Loss: 2.0942\n",
      "Epoch [250/300] Train Loss: 2.2701 Val Loss: 2.0942\n",
      "Epoch [300/300] Train Loss: 1.7825 Val Loss: 2.0942\n",
      "Training Adam_gamma0.1_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 1.7048 Val Loss: 2.2348\n",
      "Epoch [100/300] Train Loss: 1.8500 Val Loss: 2.1372\n",
      "Epoch [150/300] Train Loss: 1.5111 Val Loss: 2.1353\n",
      "Epoch [200/300] Train Loss: 1.4540 Val Loss: 2.1353\n",
      "Epoch [250/300] Train Loss: 1.9005 Val Loss: 2.1353\n",
      "Epoch [300/300] Train Loss: 1.7878 Val Loss: 2.1353\n",
      "Training Adam_gamma0.1_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 1.1237 Val Loss: 1.7271\n",
      "Epoch [100/300] Train Loss: 1.3537 Val Loss: 1.6600\n",
      "Epoch [150/300] Train Loss: 1.4461 Val Loss: 1.6586\n",
      "Epoch [200/300] Train Loss: 1.3345 Val Loss: 1.6586\n",
      "Epoch [250/300] Train Loss: 1.9069 Val Loss: 1.6586\n",
      "Epoch [300/300] Train Loss: 1.7127 Val Loss: 1.6586\n",
      "Training Adam_gamma0.001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4160 Val Loss: 0.4923\n",
      "Epoch [100/300] Train Loss: 0.3462 Val Loss: 0.4879\n",
      "Epoch [150/300] Train Loss: 0.4845 Val Loss: 0.4879\n",
      "Epoch [200/300] Train Loss: 0.5644 Val Loss: 0.4879\n",
      "Epoch [250/300] Train Loss: 0.4178 Val Loss: 0.4879\n",
      "Epoch [300/300] Train Loss: 0.3721 Val Loss: 0.4879\n",
      "Training Adam_gamma0.001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.3260 Val Loss: 0.4739\n",
      "Epoch [100/300] Train Loss: 0.2880 Val Loss: 0.4729\n",
      "Epoch [150/300] Train Loss: 0.3697 Val Loss: 0.4729\n",
      "Epoch [200/300] Train Loss: 0.3148 Val Loss: 0.4729\n",
      "Epoch [250/300] Train Loss: 0.3497 Val Loss: 0.4729\n",
      "Epoch [300/300] Train Loss: 0.3033 Val Loss: 0.4729\n",
      "Training Adam_gamma0.001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.3862 Val Loss: 0.4844\n",
      "Epoch [100/300] Train Loss: 0.2765 Val Loss: 0.4823\n",
      "Epoch [150/300] Train Loss: 0.3012 Val Loss: 0.4823\n",
      "Epoch [200/300] Train Loss: 0.3864 Val Loss: 0.4823\n",
      "Epoch [250/300] Train Loss: 0.3531 Val Loss: 0.4823\n",
      "Epoch [300/300] Train Loss: 0.2992 Val Loss: 0.4823\n",
      "Training Adam_gamma0.001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4217 Val Loss: 0.4937\n",
      "Epoch [100/300] Train Loss: 0.5161 Val Loss: 0.4935\n",
      "Epoch [150/300] Train Loss: 0.5919 Val Loss: 0.4935\n",
      "Epoch [200/300] Train Loss: 0.3973 Val Loss: 0.4935\n",
      "Epoch [250/300] Train Loss: 0.4102 Val Loss: 0.4935\n",
      "Epoch [300/300] Train Loss: 0.4448 Val Loss: 0.4935\n",
      "Training Adam_gamma0.001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.5457 Val Loss: 0.5091\n",
      "Epoch [100/300] Train Loss: 0.3926 Val Loss: 0.5093\n",
      "Epoch [150/300] Train Loss: 0.5253 Val Loss: 0.5093\n",
      "Epoch [200/300] Train Loss: 0.4494 Val Loss: 0.5093\n",
      "Epoch [250/300] Train Loss: 0.5432 Val Loss: 0.5093\n",
      "Epoch [300/300] Train Loss: 0.4574 Val Loss: 0.5093\n",
      "Training Adam_gamma0.001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.4589 Val Loss: 0.5112\n",
      "Epoch [100/300] Train Loss: 0.4663 Val Loss: 0.5110\n",
      "Epoch [150/300] Train Loss: 0.4374 Val Loss: 0.5110\n",
      "Epoch [200/300] Train Loss: 0.4485 Val Loss: 0.5110\n",
      "Epoch [250/300] Train Loss: 0.3966 Val Loss: 0.5110\n",
      "Epoch [300/300] Train Loss: 0.4276 Val Loss: 0.5110\n",
      "Training Adam_gamma0.001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 1.2964 Val Loss: 2.3401\n",
      "Epoch [100/300] Train Loss: 5.7020 Val Loss: 2.3396\n",
      "Epoch [150/300] Train Loss: 1.7653 Val Loss: 2.3396\n",
      "Epoch [200/300] Train Loss: 1.9588 Val Loss: 2.3396\n",
      "Epoch [250/300] Train Loss: 1.7287 Val Loss: 2.3396\n",
      "Epoch [300/300] Train Loss: 1.9684 Val Loss: 2.3396\n",
      "Training Adam_gamma0.001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 2.7207 Val Loss: 2.2189\n",
      "Epoch [100/300] Train Loss: 1.6967 Val Loss: 2.2183\n",
      "Epoch [150/300] Train Loss: 2.6019 Val Loss: 2.2183\n",
      "Epoch [200/300] Train Loss: 1.7095 Val Loss: 2.2183\n",
      "Epoch [250/300] Train Loss: 1.8255 Val Loss: 2.2183\n",
      "Epoch [300/300] Train Loss: 2.0414 Val Loss: 2.2183\n",
      "Training Adam_gamma0.001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 2.6087 Val Loss: 2.4114\n",
      "Epoch [100/300] Train Loss: 2.6246 Val Loss: 2.4109\n",
      "Epoch [150/300] Train Loss: 1.3658 Val Loss: 2.4109\n",
      "Epoch [200/300] Train Loss: 1.7024 Val Loss: 2.4109\n",
      "Epoch [250/300] Train Loss: 2.2356 Val Loss: 2.4109\n",
      "Epoch [300/300] Train Loss: 3.3904 Val Loss: 2.4109\n",
      "Training Adam_gamma0.0001_lr0.01_batch16...\n",
      "Epoch [50/300] Train Loss: 0.4266 Val Loss: 0.5104\n",
      "Epoch [100/300] Train Loss: 0.3948 Val Loss: 0.5081\n",
      "Epoch [150/300] Train Loss: 0.4159 Val Loss: 0.5081\n",
      "Epoch [200/300] Train Loss: 0.3690 Val Loss: 0.5081\n",
      "Epoch [250/300] Train Loss: 0.4289 Val Loss: 0.5081\n",
      "Epoch [300/300] Train Loss: 0.4959 Val Loss: 0.5081\n",
      "Training Adam_gamma0.0001_lr0.01_batch160...\n",
      "Epoch [50/300] Train Loss: 0.3812 Val Loss: 0.4908\n",
      "Epoch [100/300] Train Loss: 0.4760 Val Loss: 0.4901\n",
      "Epoch [150/300] Train Loss: 0.5298 Val Loss: 0.4901\n",
      "Epoch [200/300] Train Loss: 0.4384 Val Loss: 0.4901\n",
      "Epoch [250/300] Train Loss: 0.6355 Val Loss: 0.4901\n",
      "Epoch [300/300] Train Loss: 0.3464 Val Loss: 0.4901\n",
      "Training Adam_gamma0.0001_lr0.01_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5962 Val Loss: 0.5195\n",
      "Epoch [100/300] Train Loss: 0.4948 Val Loss: 0.5142\n",
      "Epoch [150/300] Train Loss: 0.5515 Val Loss: 0.5142\n",
      "Epoch [200/300] Train Loss: 0.4633 Val Loss: 0.5142\n",
      "Epoch [250/300] Train Loss: 0.4380 Val Loss: 0.5142\n",
      "Epoch [300/300] Train Loss: 0.5051 Val Loss: 0.5142\n",
      "Training Adam_gamma0.0001_lr0.001_batch16...\n",
      "Epoch [50/300] Train Loss: 0.3876 Val Loss: 0.5045\n",
      "Epoch [100/300] Train Loss: 0.3386 Val Loss: 0.5043\n",
      "Epoch [150/300] Train Loss: 0.5450 Val Loss: 0.5043\n",
      "Epoch [200/300] Train Loss: 0.4200 Val Loss: 0.5043\n",
      "Epoch [250/300] Train Loss: 0.4865 Val Loss: 0.5043\n",
      "Epoch [300/300] Train Loss: 0.5314 Val Loss: 0.5043\n",
      "Training Adam_gamma0.0001_lr0.001_batch160...\n",
      "Epoch [50/300] Train Loss: 0.4155 Val Loss: 0.4895\n",
      "Epoch [100/300] Train Loss: 0.4258 Val Loss: 0.4891\n",
      "Epoch [150/300] Train Loss: 0.4975 Val Loss: 0.4891\n",
      "Epoch [200/300] Train Loss: 0.3966 Val Loss: 0.4891\n",
      "Epoch [250/300] Train Loss: 0.4301 Val Loss: 0.4891\n",
      "Epoch [300/300] Train Loss: 0.6642 Val Loss: 0.4891\n",
      "Training Adam_gamma0.0001_lr0.001_batch1600...\n",
      "Epoch [50/300] Train Loss: 0.5717 Val Loss: 0.5200\n",
      "Epoch [100/300] Train Loss: 0.4058 Val Loss: 0.5194\n",
      "Epoch [150/300] Train Loss: 0.3664 Val Loss: 0.5194\n",
      "Epoch [200/300] Train Loss: 0.3925 Val Loss: 0.5194\n",
      "Epoch [250/300] Train Loss: 0.6854 Val Loss: 0.5194\n",
      "Epoch [300/300] Train Loss: 0.3494 Val Loss: 0.5194\n",
      "Training Adam_gamma0.0001_lr0.0001_batch16...\n",
      "Epoch [50/300] Train Loss: 1.9227 Val Loss: 2.0974\n",
      "Epoch [100/300] Train Loss: 1.8038 Val Loss: 2.0974\n",
      "Epoch [150/300] Train Loss: 2.3086 Val Loss: 2.0974\n",
      "Epoch [200/300] Train Loss: 1.4666 Val Loss: 2.0974\n",
      "Epoch [250/300] Train Loss: 1.8436 Val Loss: 2.0974\n",
      "Epoch [300/300] Train Loss: 2.4540 Val Loss: 2.0974\n",
      "Training Adam_gamma0.0001_lr0.0001_batch160...\n",
      "Epoch [50/300] Train Loss: 1.7534 Val Loss: 1.9912\n",
      "Epoch [100/300] Train Loss: 1.7177 Val Loss: 1.9912\n",
      "Epoch [150/300] Train Loss: 1.4168 Val Loss: 1.9912\n",
      "Epoch [200/300] Train Loss: 5.6824 Val Loss: 1.9912\n",
      "Epoch [250/300] Train Loss: 1.2660 Val Loss: 1.9912\n",
      "Epoch [300/300] Train Loss: 1.7322 Val Loss: 1.9912\n",
      "Training Adam_gamma0.0001_lr0.0001_batch1600...\n",
      "Epoch [50/300] Train Loss: 2.1366 Val Loss: 2.1137\n",
      "Epoch [100/300] Train Loss: 1.8508 Val Loss: 2.1136\n",
      "Epoch [150/300] Train Loss: 1.9761 Val Loss: 2.1136\n",
      "Epoch [200/300] Train Loss: 4.9027 Val Loss: 2.1136\n",
      "Epoch [250/300] Train Loss: 1.4694 Val Loss: 2.1136\n",
      "Epoch [300/300] Train Loss: 1.3050 Val Loss: 2.1136\n",
      "SGD_gamma0.1_lr0.01_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.01_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.01_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.0001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.0001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.1_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.01_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.01_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.001_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.01_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.01_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "SGD_gamma0.0001_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.01_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.01_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.01_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.0001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.0001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.1_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.01_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.01_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.001_lr0.0001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.01_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.01_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.01_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.001_batch1600 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.0001_batch16 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.0001_batch160 Test Loss: 0.4749\n",
      "Adam_gamma0.0001_lr0.0001_batch1600 Test Loss: 0.4749\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define a function for training and evaluating the model with different hyperparameters\n",
    "def train_evaluate(model, optimizer_name, gamma, learning_rate, batch_size, train_loader, val_loader):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=gamma)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 300\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "        train_loss_history.append(loss.item())\n",
    "        val_loss_history.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {loss.item():.4f} Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Define hyperparameter combinations to explore\n",
    "optimizers = ['SGD', 'Adam']\n",
    "gammas = [0.1, 0.001, 0.0001]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "batch_sizes = [16, 160, 1600]\n",
    "\n",
    "# Perform the experiment\n",
    "results = {}\n",
    "best_loss = 100\n",
    "for optimizer_name in optimizers:\n",
    "    for gamma in gammas:\n",
    "        for learning_rate in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                key = f\"{optimizer_name}_gamma{gamma}_lr{learning_rate}_batch{batch_size}\"\n",
    "                print(f\"Training {key}...\")\n",
    "                model = WineQualityRegressor(input_size, hidden_size)\n",
    "                train_loss, val_loss = train_evaluate(model, optimizer_name, gamma, learning_rate, batch_size, train_loader, val_loader)\n",
    "                if val_loss[-1] < best_loss:\n",
    "                    best_loss = val_loss[-1]\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                results[key] = {\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss\n",
    "                }\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_losses = {}\n",
    "for key, result in results.items():\n",
    "    model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "    test_losses[key] = test_loss.item()\n",
    "\n",
    "# Print test losses\n",
    "for key, test_loss in test_losses.items():\n",
    "    print(f\"{key} Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6f267-8bbd-48bd-95c3-22a318e326e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40b7f3a-9143-44fb-9438-eb37776bde09",
   "metadata": {},
   "source": [
    "![q](pic/q4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e77e4-0ee2-4eda-a602-4e6bdd9e3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrained_model.Encoder import extractor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb4dbd-b68e-47cf-92a2-394ca76b0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "m = 256\n",
    "k = 10\n",
    "n_epochs = 30\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237ee07-965a-4969-97ee-5b60e14722b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc4fe0-ab2c-4a28-a3e6-23ce638067f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8260ea6-8dfc-4a41-a755-67c5dfcb07f3",
   "metadata": {},
   "source": [
    "![q](pic/q5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad03736-8c31-435f-9c48-1af83235b733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE685D",
   "language": "python",
   "name": "ece685d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
