{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad9e035",
   "metadata": {},
   "source": [
    "# Part 5 DNN and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228a1d8",
   "metadata": {},
   "source": [
    "### The material presented in this notebook is for using in Introduction to Deep Learning (ECE 685D) course, Duke University, Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ffa2d",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58096ba",
   "metadata": {},
   "source": [
    "### Define a deep neural network (DNN) in Pytorch\n",
    "    a. an nn.Mudule class\n",
    "    b. Define the layers in __init__()  \n",
    "    c. Define the forward pass in forward()\n",
    "    \n",
    "Example of a CNN model:\n",
    "\n",
    "    class Network(nn.Module):\n",
    "        def __init__(self, parameters):\n",
    "            super().__init__()\n",
    "            # your layers defined here\n",
    "            self.layer1 = ...\n",
    "            \n",
    "        def forward(x):\n",
    "            # your forward pass defined here\n",
    "            output = self.layer1(x)\n",
    "            ...\n",
    "            return output\n",
    "\n",
    "\n",
    "    model = Network() #create a model  \n",
    "    model(data)       #forward pass with your data\n",
    "    model.backward()  #backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc746e",
   "metadata": {},
   "source": [
    "## 1. Different ways to define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0186c77",
   "metadata": {},
   "source": [
    "### a. Directly define the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e995cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e15868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network1(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.first_conv = nn.Conv2d(in_channels, 32, 3, 1)\n",
    "        self.conv1 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pooling = nn.MaxPool2d(2)\n",
    "        #you may need to change the numbers when given an input of different dimensions\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, out_channels)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.first_conv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pooling(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        # flatten the output to a vector for classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9940d",
   "metadata": {},
   "source": [
    "#### Problem: heavy codes and low readability if the layer number further goes up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429fa2",
   "metadata": {},
   "source": [
    "### b. use nn.Sequential() to define blocks\n",
    "nn.Sequential(): A sequential container. Modules will be added to it **in the order** they are passed in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ce63320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.first_conv = nn.Conv2d(in_channels, 32, 3, 1)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(9216, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        # flatten the output to a vector for classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d150fd",
   "metadata": {},
   "source": [
    "**problem**: still heavy codes in __init__ function when defining the containers  \n",
    "\n",
    "What if we want to define **multiple blocks with the same layers** (but different shape, e.g., channels)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bef0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## say if we need to define a few blocks of (Conv + Relu + Conv + Rely + Maxpooling)\n",
    "\n",
    "class Network2(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.first_conv = nn.Conv2d(in_channels, 32, 3, 1)\n",
    "        \n",
    "        self.feature_extractor1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        self.feature_extractor2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        self.feature_extractor3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        #...\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(9216, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor1(x)\n",
    "        x = self.feature_extractor2(x)\n",
    "        x = self.feature_extractor3(x)\n",
    "        # flatten the output to a vector for classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292f07a",
   "metadata": {},
   "source": [
    "Codes are too long!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac785c8",
   "metadata": {},
   "source": [
    "### c. Use functions and modules to make the codes even more concise and readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0af4a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define modules\n",
    "class feature_extractor(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, 3, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(0.25),\n",
    "            )\n",
    "    def forward(x):\n",
    "        return self.feature_extractor(x)\n",
    "    \n",
    "    \n",
    "class Network3(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.first_conv = nn.Conv2d(in_channels, 32, 3, 1)\n",
    "        self.feature_extractor1 = feature_extractor(32, 64)\n",
    "        self.feature_extractor2 = feature_extractor(64, 128)\n",
    "        self.feature_extractor3 = feature_extractor(128, 256)\n",
    "        #...\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(9216, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.feature_extractor1(x)\n",
    "        x = self.feature_extractor2(x)\n",
    "        x = self.feature_extractor3(x)\n",
    "        # flatten the output to a vector for classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6104ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## or use modules + container\n",
    "\n",
    "class Network4(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.first_conv = nn.Conv2d(in_channels, 32, 3, 1)\n",
    "        self.feature_extractors = nn.Sequential(\n",
    "            feature_extractor(32, 64),\n",
    "            feature_extractor(64, 128),\n",
    "            feature_extractor(128, 256),\n",
    "            #...\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(9216, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.feature_extractors(x)\n",
    "        # flatten the output to a vector for classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4feecd",
   "metadata": {},
   "source": [
    "## 2. Forward pass\n",
    "    In forward pass, we define the exact order of how we input the data into the network.  \n",
    "    The order of operation does not need to follow the layer definition order in __init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network4(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## the order of definition doesn't matter here\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(9216, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.feature_extractors = nn.Sequential(\n",
    "            feature_extractor(32, 64),\n",
    "            feature_extractor(64, 128),\n",
    "            feature_extractor(128, 256),\n",
    "            #...\n",
    "            ## the order in nn.Sequential matters\n",
    "        )\n",
    "        \n",
    "        self.first_conv = nn.Conv2d(in_channels, 32, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## the order does matter here\n",
    "        output = self.first_conv(x)\n",
    "        output = self.feature_extractors(output)\n",
    "        # flatten the output to a vector for classification\n",
    "        output = torch.flatten(output, 1)\n",
    "        \n",
    "        output = self.classifier(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a0965",
   "metadata": {},
   "source": [
    "**Sometimes we can use torch.nn.functional to implement some functions without claiming them in the __ init__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32365180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two are equivalent:\n",
    "\n",
    "## method 1\n",
    "# relu_layer = nn.ReLU()\n",
    "# out = relu_layer(x)\n",
    "\n",
    "# # method 2\n",
    "# out = F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975cdd5",
   "metadata": {},
   "source": [
    "**Differences in forward pass between training mode and evaluation mode**\n",
    "\n",
    "You might notice that in the previous example we set model.train() before every training and set model.eval() before each testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32513a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### codes from Part 4\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = 0\n",
    "    model.train() # what is the training mode?\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % (len(train_loader)//2) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    model.eval() # what is the eval mode?\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}, Accuracy: {:.4f}%'.format(\n",
    "        epoch, test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29377883",
   "metadata": {},
   "source": [
    "## Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf3550",
   "metadata": {},
   "source": [
    "**Not every layer is needed in evaluation!**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c22f29",
   "metadata": {},
   "source": [
    "**What layers are excluded from testing process?**\n",
    "- Dropout: we use dropout in training to prevent overfitting by randomly cut some nodes at each iteration. But in testing, we use all the nodes.\n",
    "![dropout Operation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*dEi_IkVB7IpkzZ-6H0Vpsg.png)\n",
    "\n",
    "**What layers are operated differently in the testing process?**\n",
    "- Batch Normalization: In training, we use **per-batch** statistics. In testing, we use the **global** running statistics\n",
    "\n",
    "**Conclusion: don't forget to switch mode when training/testing your CNN!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea35a03",
   "metadata": {},
   "source": [
    "# Back-propagation\n",
    "In pytorch, we don't need to define a backward pass function.\n",
    "Simply call .backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9ad34",
   "metadata": {},
   "source": [
    "### when do we need backpropagation?\n",
    "- Back-propagation = get gradients\n",
    "- We need gradients when we need to update the parameters of something\n",
    "\n",
    "**(Most of the cases) We won't need gradients and backpropagation in testing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbc954",
   "metadata": {},
   "source": [
    "We can turn off the gradient calculation in testing by\n",
    "\n",
    "    1. use with torch.no_grad() before testing\n",
    "    2. don't call backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6f4b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    model.eval() # what is the eval mode?\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}, Accuracy: {:.4f}%'.format(\n",
    "        epoch, test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9ec2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316062fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
